%\VignetteIndexEntry{rsae: Robust Small Area Estimation}
\documentclass[a4paper,oneside,11pt,DIV=12]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\renewcommand{\familydefault}{\rmdefault}
\setlength\parindent{24pt}

\usepackage{SweaveCOLOR}
\setkeys{Gin}{width = 0.8\textwidth}

\usepackage{color}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{url}

\usepackage{setspace}
\usepackage[longnamesfirst]{natbib}

\setkomafont{captionlabel}{\sffamily\bfseries\small}
\setkomafont{caption}{\sffamily\small}

\usepackage{amssymb}
\usepackage{amsmath}

% position figure 't' on an empty page
\makeatletter
    \setlength\@fptop{0\p@}
\makeatother

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,
    urlcolor=blue,
    citecolor=blue
}

%================================================
% Greek vectors and matrices
   \newcommand{\valpha}{\boldsymbol{\alpha}}
   \newcommand{\vbeta}{\boldsymbol{\beta}}
   \newcommand{\vtheta}{\boldsymbol{\theta}}
   \newcommand{\vgamma}{\boldsymbol{\gamma}}
   \newcommand{\vdelta}{\boldsymbol{\delta}}
   \newcommand{\vepsilon}{\boldsymbol{\epsilon}}
   \newcommand{\veta}{\boldsymbol{\eta}}
   \newcommand{\viota}{\boldsymbol{\itoa}}
   \newcommand{\vkappa}{\boldsymbol{\kappa}}
   \newcommand{\vlambda}{\boldsymbol{\lambda}}
   \newcommand{\vmu}{\boldsymbol{\mu}}
   \newcommand{\vnu}{\boldsymbol{\nu}}
   \newcommand{\vxi}{\boldsymbol{\xi}}
   \newcommand{\vpi}{\boldsymbol{\pi}}
   \newcommand{\vrho}{\boldsymbol{\rho}}
   \newcommand{\vsigma}{\boldsymbol{\sigma}}
   \newcommand{\vtau}{\boldsymbol{\tau}}
   \newcommand{\vpsi}{\boldsymbol{\psi}}
   \newcommand{\vphi}{\boldsymbol{\phi}}
   \newcommand{\vchi}{\boldsymbol{\chi}}
   \newcommand{\vomega}{\boldsymbol{\omega}}
%
   \newcommand{\matalpha}{\boldsymbol{\Alpha}}
   \newcommand{\matbeta}{\boldsymbol{\Beta}}
   \newcommand{\mattheta}{\boldsymbol{\Theta}}
   \newcommand{\matgamma}{\boldsymbol{\Gamma}}
   \newcommand{\matdelta}{\boldsymbol{\Delta}}
   \newcommand{\matepsilon}{\boldsymbol{\Epsilon}}
   \newcommand{\mateta}{\boldsymbol{\Eta}}
   \newcommand{\matiota}{\boldsymbol{\Itoa}}
   \newcommand{\matkappa}{\boldsymbol{\Kappa}}
   \newcommand{\matlambda}{\boldsymbol{\Lambda}}
   \newcommand{\matmu}{\boldsymbol{\Mu}}
   \newcommand{\matnu}{\boldsymbol{\Nu}}
   \newcommand{\matxi}{\boldsymbol{\Xi}}
   \newcommand{\matpi}{\boldsymbol{\Pi}}
   \newcommand{\matrho}{\boldsymbol{\Rho}}
   \newcommand{\matsigma}{\boldsymbol{\Sigma}}
   \newcommand{\matau}{\boldsymbol{\Tau}}
   \newcommand{\matpsi}{\boldsymbol{\Psi}}
   \newcommand{\matphi}{\boldsymbol{\Phi}}
   \newcommand{\matchi}{\boldsymbol{\Chi}}
   \newcommand{\matomega}{\boldsymbol{\Omega}}

% Latin vectors
   \newcommand{\va}{\mathbf{a}}
   \newcommand{\vb}{\mathbf{b}}
   \newcommand{\vc}{\mathbf{c}}
   \newcommand{\vd}{\mathbf{d}}
   \newcommand{\ve}{\mathbf{e}}
   \newcommand{\vf}{\mathbf{f}}
   \newcommand{\vg}{\mathbf{g}}
   \newcommand{\vh}{\mathbf{h}}
   \newcommand{\vj}{\mathbf{i}}
   \newcommand{\vi}{\mathbf{j}}
   \newcommand{\vk}{\mathbf{k}}
   \newcommand{\vl}{\mathbf{l}}
   \newcommand{\vm}{\mathbf{m}}
   \newcommand{\vn}{\mathbf{n}}
   \newcommand{\vo}{\mathbf{n}}
   \newcommand{\vp}{\mathbf{p}}
   \newcommand{\vq}{\mathbf{q}}
   \newcommand{\vr}{\mathbf{r}}
   \newcommand{\vs}{\mathbf{s}}
   \newcommand{\vt}{\mathbf{t}}
   \newcommand{\vu}{\mathbf{u}}
   \newcommand{\vv}{\mathbf{v}}
   \newcommand{\vw}{\mathbf{w}}
   \newcommand{\vx}{\mathbf{x}}
   \newcommand{\vy}{\mathbf{y}}
   \newcommand{\vz}{\mathbf{z}}

% Specital vector
   \newcommand{\vone}{\mathbf{1}}  % vector of ones
   \newcommand{\vzero}{\mathbf{0}}  % vector of zeros

% Latin matrices
   \newcommand{\mata}{\mathbf{A}}
   \newcommand{\matb}{\mathbf{B}}
   \newcommand{\matc}{\mathbf{C}}
   \newcommand{\matd}{\mathbf{D}}
   \newcommand{\mate}{\mathbf{E}}
   \newcommand{\matf}{\mathbf{F}}
   \newcommand{\matg}{\mathbf{G}}
   \newcommand{\matH}{\mathbf{J}}
   \newcommand{\mati}{\mathbf{I}}
   \newcommand{\matj}{\mathbf{J}}
   \newcommand{\matk}{\mathbf{K}}
   \newcommand{\matl}{\mathbf{L}}
   \newcommand{\matm}{\mathbf{M}}
   \newcommand{\matn}{\mathbf{N}}
   \newcommand{\mato}{\mathbf{O}}
   \newcommand{\matp}{\mathbf{P}}
   \newcommand{\matq}{\mathbf{Q}}
   \newcommand{\matr}{\mathbf{R}}
   \newcommand{\mats}{\mathbf{S}}
   \newcommand{\matt}{\mathbf{T}}
   \newcommand{\matu}{\mathbf{U}}
   \newcommand{\matv}{\mathbf{V}}
   \newcommand{\matw}{\mathbf{W}}
   \newcommand{\matx}{\mathbf{X}}
   \newcommand{\maty}{\mathbf{Y}}
   \newcommand{\matz}{\mathbf{Z}}

%================================================

\title{Robust Small Area Estimation: a Vignette}
\author{Tobias Schoch\thanks{Contact: Tobias Schoch, University of Applied
    Sciences Northwestern Switzerland, Riggenbachstrasse 16, CH-4600 Olten,
    Switzerland; e-mail: tobias.schoch@fhnw.ch; phone: +41 62 286 01 47.}}
\date{December 29, 2011: rsae 0.1-4}

\begin{document}
\maketitle

\tableofcontents

%================================================
\section{Introduction}
\setstretch{1.1}

The \texttt{rsae} package offers functions to estimate

\begin{itemize}
    \item model A, area-level SAE model, aka Fay-Herriot model,
    \item model B, basic unit-level SAE model,
\end{itemize}

\noindent by either maximum likelihood or robust methods. The classification
of the models (A or B) follows the proposal of \citet{rao2003}. Currently,
the package includes the following robust (i.e. bounded-influence estimating
equation) methods for model A:

\begin{itemize}
    \item Huber-type $M$-estimation, using a slight generalization of
        Huber's Proposal 2 to estimate the variance components; cf.,
        \citet{richardsonwelsh1995}, \citet{sinharao2009}, \citet{schoch2011},
    \item variance estimation and MSPE estimation (parametric bootstrap;
        see \citet{sinharao2009})
    %\item \textit{robust high-breakdown point $S$-estimation [testing phase,
    %     expected release for SAE conference 2011]},
    %\item \textit{robust simple and minimal-iterative methods using fast-LTS
    %    and a MED-MAD strategy.[under development]}
\end{itemize}

\noindent (Robust estimating methods for model type "a" will be released
in the near future)

\noindent \textbf{NOTE:} In order to use the full functionality of
\textbf{rsae}, I recommend to install the \textbf{robustbase} package.

%================================================
\section{Getting started}
I will not attempt to provide another introduction to \texttt{R}. There
are several excellent resources intended to accomplish this task.

Once \texttt{R} is running, the installation of additional packages is
straightforward. A platform-independent way to install the \texttt{rsae}
package is to type

<<echo=TRUE, eval=FALSE>>=
install.package("rsae")
@

\noindent in the console (note that the character "\texttt{>}" is not part
of the command; it denotes the command prompt of the command-line
interface, indicating that \texttt{R} is ready to read your commands).
Provided your computer has a proper internet connection (and you have
sufficient writing permissions on your system), the installation of the
package should proceed automatically (at least on Windows and Apple
computers, because binary sources are available from CRAN). On POSIX
compliant OS, e.g., AIX, HP-UX, Solaris, etc. and mostly POSIX-compliant
systems such as Linux, OpenSolaris, etc. installation is more involved
(see Appendix \ref{sec:os} for more details on the installation process).


Once the \texttt{rsae} package is installed, we need to load it to the
current session.\footnote{Note that we have to load a package in every
session where we need its functionality, whereas installation is needed
a single time on your computer.}

<<>>=
library(rsae)
@


\noindent The model fitting exercise with \texttt{rsae} takes alway
three (or more) steps. Namely,
\begin{itemize}
    \item setting up a \texttt{saemodel}, given some data,
    \item fitting the \texttt{saemodel} by robust methods,
    \item (robustly) predicting the random effects and the area-specific
        means, given the robustly fitted model.
\end{itemize}

\noindent These steps will be discussed in subsequent Sections.

%================================================
\section{Setting up a model}
First of all, we have to set up a model. We use the \texttt{landsat}
data from \citet{batteseetal1988}, the landmark paper on the basic
unit-level SAE model (here: units=segments, each about 250 hectares;
areas=Counties). Those readers who are not familiar with the
\texttt{landsat} can type \texttt{help(landsat)} in the \texttt{R}
console in order to get a description of the data.


%------------------------------------------------
\subsection{Exploring the data}
First, we have to load the \texttt{landsat} data into the workspace.

<<>>=
data(landsat)
@

\noindent Next, we will explore the data. The \texttt{names} command
reports the names of the variables in the \texttt{landsat} data.
<<>>=
names(landsat)
@

\noindent Get a table (i.e., an aggregation) of the variable
\texttt{CountyName}.
<<>>=
table(landsat$CountyName)
@

\noindent So, there are twelve areas -- the smallest areas (Cerro Gordo,
Hamilton, and Worth) contain one unit, the largest area, Hardin,
involves six units.

Next, we want to have a look at observations no. 30 to 35 and some
interesting variables.

<<>>=
landsat[30:35, c("HACorn", "PixelsCorn", "PixelsSoybeans",
    "CountyName", "outlier")]
@

\noindent Note that I added the variable \texttt{outlier} to the original
data. This variable flags observation no. 33 as outlier. This is in
line with \citet{batteseetal1988}, who indicate that for both observations
no. 32 and 33, the interviewed farm operators reported for \texttt{HACorn}
the same value. Despite the same number of estimated hectares under
corn (i.e., 88.59), the readings for \texttt{PixelsCorn} are very different
for obs. no. 32 and 33. To be precise, we would call observation no. 33 a
leverage point rather than an outlier.

<<fig=TRUE, echo=FALSE>>=
    linmodel <- lm(HACorn ~ PixelsCorn + PixelsSoybeans, data = landsat)
    plot(linmodel, 5)
@


The following figure is a display of the (standardized) residuals of the
linear model against the leverage. Note that obs. no. 33 is very close
the the bound on the values of Cook's distance.

<<fig=TRUE, eval=FALSE>>=
    linmodel <- lm(HACorn ~ PixelsCorn + PixelsSoybeans, data = landsat)
    plot(linmodel, 5)
@

Note that \citet{sinharao2009}, on the other hand, included this bad
leverage point in their simulation exercise and obtained completely
different estimates.

%------------------------------------------------
\subsection{We set up our first model}

Having explored the data, we consider setting up the model. The
(first of their) model writes \citep{batteseetal1988}

\begin{equation}
    HACorn_{i,j} = \alpha + \beta_1 \cdot PixelsCorn_{i,j}
    + \beta_2 \cdot PixelsSoybeans_{i,j} + u_i + e_{i,j}, \nonumber
\end{equation}

\noindent where $j=1,\ldots, n_i$,  $i=1, \ldots,12$.

The \texttt{rsae} package provides the function \texttt{saemodel} to set
up a model. The model is specified by mainly three arguments: (1) the
\texttt{formula} argument defines the fixed-effect part (the $\sim$
operator separates dependent and independent variables), (2) the
\texttt{area} argument specifies the area-level random effect (here,
the variable \texttt{CountyName} serves as area identifier; note the
leading $\sim$ operator), (3) the \texttt{data} argument specifies the
data (a \texttt{data.frame}). The only "difficulty" here is that we use
\texttt{data=subset(landsat, subset=(outlier==FALSE))} to include only
non-outlying observations, instead of \texttt{data=landsat}. We call
our model \texttt{bhfmodel} (Batteese, Harter, and Fuller).

<<>>=
bhfmodel <- saemodel(formula=HACorn ~ PixelsCorn + PixelsSoybeans,
                    area=~CountyName,
                    data=subset(landsat, subset=(outlier==FALSE)))
@

\noindent (Note that in \texttt{R}, a \texttt{formula object} (evaluated
by \texttt{model.frame} and \texttt{model.matrix}) contains always an
intercept term. In the case we want to fit the above model without an
intercept, we have to change the RHS of the formula part to look like
this: \texttt{-1 + PixelsCorn + PixelsSoybeans}).

Suppose we have generated several different models (e.g., using different
independent variables), and they all reside in the current workspace.
It may be difficult to figure out which of them is the \texttt{bhfmodel}
(except you you have adopted unique naming conventions). For situations
like this, type the name of the model to get some information.

<<>>=
bhfmodel
@

If you need to know more about a particular model, you may use the
\texttt{summary} method.

<<>>=
summary(bhfmodel)
@

\noindent The important thing to note is that \texttt{summary} reminds
you of the fact that the model was generated using only non-outlying
observations.

%================================================
\section{(Robust) Estimation}
Having set up our model, we consider estimating the parameters of the
Gaussian core model by different methods. All fitting is done using the
following workhorse function

\begin{verbatim}
fitsaemodel(method, model, ...)
\end{verbatim}

\noindent Depending on the arguments delivered, \texttt{fitsaemodel}
decides what method to use. The decision is based on the \texttt{method}
argument.

%------------------------------------------------
\subsection{Maximum likelihood estimation}
To start with, we compute the usual maximum likelihood (ML) estimates.
Therefore, we call \texttt{fitsaemodel} with \texttt{method="ml"}
(the ML methods does not need any additional [\texttt{...}] arguments).

<<>>=
mlfit <- fitsaemodel("ml", bhfmodel)
@

\noindent Type name of the fit, i.e. \texttt{mlfit}, to get a display of
the model fit.
<<>>=
mlfit
@

\noindent Note that the layout of the output is similar to the one
of the \texttt{lme} (linear mixed-effects model) function in the
\texttt{nlme} package \citep[cf.][]{pinheirobates2011}. In particular,
the output on the random-effects part mimicks what the \texttt{print}
methods reports for \texttt{lme} (i.e., \texttt{$\sim$1|Countyname}).

To learn more about the fit, we may call the \texttt{summary} method.
In particular, the model summary supplies us with inferential statistics
of fixed effects.

<<>>=
summary(mlfit)
@

%------------------------------------------------
\subsubsection{When the ML estimator does not converge}
Now, we deliberately set the number of outer-loop iterations to one,
in order to show the model report, when the algorithm fails to converge.

<<>>=
failconvg <- fitsaemodel("ml", bhfmodel, niter=1)
@

\noindent Here is the report

<<>>=
failconvg
@

\noindent Obviously, \texttt{niter=1} is a bad choice. Nontheless, we
follow the hint and call the \texttt{convergence} method:

<<>>=
convergence(failconvg)
@

\noindent Now, it may happen that the ML-method of \texttt{fitsaemodel}
does not converge (even if the parameters have not been modified). A
remedy is to initialize the ML method by a regression $S$-estimator
(sic!), calling \texttt{fitsaemodel} with \texttt{init="s"}. Obviously,
this approach is not optimal in terms of computing time. Nonetheless,
by this specification, \texttt{fitsaemodel} enters the "safe mode" of
robust Huber-type $M$-estimation and applies several checks whether the
algorithm behaves well (these checks are ignored in the default mode).

%------------------------------------------------
\subsubsection{When the mixed linear model is not appropriate}
Suppose that our data do not have area-specific variation. Notably,
we shall generate data based on the linear model,

\begin{equation}
   y_{i,j} =  (1, \; x_{i,j})^T\vbeta + e_{i,j},
\end{equation}

\noindent where $x_{i,j} \sim N(0,1)$, $e_{i,j}\sim N(0,1)$,
$\vbeta=(1,1)^T$, $n_i=n=10$, $\forall i=1, \ldots,10$ (balanced data).
The following code generates the data.

<<>>=
set.seed(12345)
n <- 200; beta <- c(1, 1)
cst <- rep(1, n)
x <- rnorm(n)
y <- as.matrix(cbind(cst, x)) %*% beta + rnorm(n)
areaid <- rep(1:10, each=10)
df <- data.frame(y=y, x=x, areaid=areaid)
@

\noindent The OLS fit of our linear model is
<<>>=
lm(y ~ x, data=df)
@

\noindent Then, we set up the \texttt{saemodel} with the identical
data (i.e., \texttt{df})
<<>>=
fakemlm <- saemodel(y ~ x, area=~areaid, data=df)
@

\noindent and consider the following model fit

<<>>=
fitsaemodel("ml", fakemlm)
@

\noindent The report indicates that the random-effect variance is close
to zero or equal to zero. Therefore, the MLM model is not appropriate.

%------------------------------------------------
\subsection{Huber-type $M$-estimation}
Huber-type $M$-estimation is the recommended estimation method for
situations where the response variable is supposed to be (moderately)
contaminated. The $M$-estimators downweight the influence of outlying
observations (in the vector of responses) on the estimates. Notably,
they bound the influence of outliers. But, no attempt is made to limit
the effect of leverage points (see above). In principle, one may adapt
generalized regression $M$-estimators (GM) to the family of linear
mixed-level models in order to deal with influential observations/leverage
(i.e., to bound also the influence of the design matrix). This has been
done by \citet{richardson1997}. However, in terms of numerical properties,
the Schweppe- and Mallows-type weighted $GM$-estimators turned out to be
extremely unstable \citep{richardson1995}. Therefore, we propose to
use the $S$-estimator (with constrained parameter space) if the data
are heavily contaminated and/or contain influential observations/leverage
(see below).

Next, we discuss two different fitting modes for the Huber-type
$M$-estimation method.

\begin{itemize}
    \item If the reponse variable is supposed to be uncontamined or
        contaminated by only a couple of outliers, I recommend to use the
        (very fast) \textbf{default mode}.
   \item If the response variable is moderately contaminated and/or if
        the \textbf{default-mode algorithm failed to converge}, I recommend
        the \textbf{safe mode}.
\end{itemize}

%------------------------------------------------
\subsubsection{Default mode}
The default-mode setup of the Huber-type $M$-estmation exercise is

<<>>=
huberfit <- fitsaemodel("huberm", bhfmodel, k=1.5)
@

\noindent where \texttt{k} denotes the robustness-tuning constant of
the Huber $\psi$-function ($0<k \leq \infty$; note that (in the limit)
$k=\infty$ leads to the ML estimates). (\textbf{NOTE:} that in the
simple location-scale model, the choice of \texttt{k=1.345} leads to
estimates which feature an asymptotic relative efficiency w.r.t. the
ML estimate of approx. 95\% at the true (uncontamined) Gaussian core
model. This property does \textbf{not} directly carry over to the
estimates of mixed-linear models!)

<<>>=
huberfit
@

To learn more about the fit, we shall call the \texttt{summary} method.

<<echo=TRUE>>=
summary(huberfit)
@


\noindent The output is by default separated into 2 blocks when the
algorithm converged.
\begin{itemize}
    \item Block 1 reports some inferential statistics of the fixed-effects
        part.
    \item Block 2 reports the degree of down-weighting outlying residuals
        at the final iteration. The degree of down-weighting is reported
        for all estimating equations (EE) separately. In general $d=1$,
        if no down-weighting took place. Conversely, the smaller $d$,
        the more outlying residuals have been down-weighted (and/or
        the same outliers are heavier down-weighted).
\end{itemize}

In addition, the \texttt{convergence} method supplies us with a report
on the convergence of the method.

<<>>=
convergence(huberfit)
@

\noindent It consist of the following blocks:

\begin{itemize}
    \item Block 1 reports the default or user-specified max number of
        iterations, \texttt{niter}, and the numeric tolerance, \texttt{acc},
        used in the termination rule of the algorithms.
    \item Block 2 reports the number of iterations that each of the
        estimating equation--specific (EE-specific) loops (aka inner loops)
        and the overall loop (aka outer loop) used to conform to the
        termination rule. Each row in the table represents a single run
        of the overall loop. In the example, the algorithm needed 7
        overall loops. The entries of a row refer to the EE-specific
        number of iterations in a particular overall loop.

        It is evident that the number of (inner-loop) iterations for
        \texttt{fixeff} and \texttt{residual var} become smaller, the
        higher the number of outer/overall loops. This is not (and will
        never be) the case for \texttt{area raneff var} because it is
        obtained using a different optimization method.

        In general, we can feel confident if the number of inner-loop
        iterations decrease (in the case of \texttt{fixeff} and
        \texttt{residual var}). However, there are situations where
        the algorithm converges perfectly without featuring such a nice
        decrease in the number of inner-loop iterations.
\end{itemize}

%------------------------------------------------
\subsubsection{Safe mode}
The safe mode should be used when the data are supposed to be
moderately contaminated and/or the algorithm in default mode failed to
converge. The safe-mode algorithm is initialized by a high-breakdown-point
regression estimator. However, it is only safe up to a certain degree
of contamination (i.e., breakdown-point is rather low for $M$-estimators).
In the presence of too many outliers, the $M$-estimator will break down.

~\\
\noindent \textbf{NOTICE:} in order to use the safe-mode functions, you
need the \textbf{robustbase} package \citep{robustbase}.
~\\


The safe mode is entered if one specifies (in \texttt{fitsaemodel}) either

\begin{itemize}
    \item \texttt{init = "lts"} for initializing the method by a
        fast-LTS regression estimator
        \citep{rousseeuw1984,rousseeuwvandriessen2006},
    \item \texttt{init = "s"} for initializing the method by a
        regression $S$-estimator
        \citep{rousseeuwyohai1984,salibianbarerrayohai2006}.
\end{itemize}

\noindent(Also, the safe mode uses a couple of tests to check whether
the estimates at consecutive iterations behave well. Notably, it
prevents cycling estimates (i.e., the situation when the algorithm
is trapped in a flip-flop), which typically occurs for very small
robustness-tuning constants).

In general, the results of \texttt{fitsaemodel} with either
\texttt{init = "s"} or \texttt{init = "lts"} are the same. For data
with more than 50,000 observations, \texttt{init="s"} is considerably faster.

\noindent The call is

<<>>=
fitsaemodel("huberm", bhfmodel, k=1.2, init="s")
@

\noindent Also, we can get more information calling the \texttt{summary} method.

%------------------------------------------------
\subsection{$S$-estimation}
[has to be written]


%================================================
\section{Robust prediction}
Once the parameters of the Gaussian core model have been estimated
robustly, we consider (robustly) predicting the random effects.
\citet{sinharao2009} proposed to solve the robustified mixed model
equations \citep{fellner1986} by a Newton-Raphson-type updating scheme
that is obtained from a Taylor-series expansion of the robustified
mixed model equations. Consequently, computation is very involved.

However, \citet{schoch2011} showed that robust predictions of the
area-specifc means can be obtained far more easily by means of the
robust predictor of random effects due to \citet{coptvictoriafeser2009};
see also \citet[113--114][]{heritieretal2009}. Namely, since the
robustly estimated parameters of the core Gaussian model determine
the model completely, prediction is straightforward.

The workhorse function for (robust) prediction is given by

\begin{verbatim}
robpredict(fit, areameans=NULL, k=NULL, reps=NULL)
\end{verbatim}

\noindent where \texttt{fit} is a fitted model (an object produced
by \texttt{fitsaemodel}),  \texttt{k} is the robustness-tuning constant
(of the Huber $\psi$-function) for robust prediction. By default
\texttt{k} is \texttt{NULL} which means that the procedure takes the
same \texttt{k} as has been used for estimating the parameters of the
core model. The robustness-tuning constant \texttt{k} does not
necessarily be the same as the one used in \texttt{fitsaemodel}.

Further, choosing \texttt{k} sufficiently large (e.g., \texttt{k=20000}
should work in (almost) all situations), \texttt{robpredict} produces
the usual EBLUP predictions.

If \texttt{areameans=NULL} (the default setting), the prediction is
based on the same data that have been used for the model fitting
exercise (i.e., within-sample prediction). However, in the SAE context,
we are usually interested in (robustly) predicting the small-area
means. Therefore, we deliver the area-specific population means
through \texttt{areameans}.

In addition, the \texttt{robpredict} function can compute
area-specific mean squared prediction errors (MSPE) by means of a
(robust) parametric bootstrap method; see \citet{sinharao2009}.
In order to obtain MSPE, we need to specify the number of bootstrap
replicates, \texttt{reps}.

\noindent \textbf{NOTICE:} I recommend to start with relatively small
values of \texttt{reps}, e.g., \texttt{reps=100}, since large values
of \texttt{reps} are associated with a large computational effort. Once
you get an intuition of how much time the algorithm consumes, you can
try larger values of \texttt{reps}.

In the \texttt{landsat} example, the county-specific population means
of pixels of the segments under corn and soybeans are recorded in the
variables \texttt{MeanPixelsCorn} and \texttt{MeanPixelsSoybeans},
respectively. Note that each sample segment in a particular county has
assigned the county-specific mean (in the \texttt{landsat} data).
Therefore, each population mean of the variables \texttt{MeanPixelsCorn}
and \texttt{MeanPixelsSoybeans} occurs $n_i$ times. The unique
county-specific population means are obtained using

<<>>=
d <- unique(landsat[-33, c("MeanPixelsCorn", "MeanPixelsSoybeans",
    "CountyName")])
d <- cbind(rep(1,12), d)
rownames(d) <- d$CountyName
d <- d[,1:3]
@

\noindent Let us have a look at \texttt{d}.
<<>>=
d
@

\noindent The first column of \texttt{d} is a dummy variable
(the \texttt{bhfmodel} has an intercept term). The second and third
column represent the county-specific population means of segments
under corn and soybeans (the rows of the above table are labeled
with the county names).

Next, we consider predicting the random and fixed effects and the
county means. Note that we do not explicitly specify \texttt{k}. This
means that the procedure uses the same \texttt{k} as the one that has
been used for robust estimation (here, the model has been estimated
by \texttt{method="ml"} which is equivalent to $k=\infty$). MSPE is
obtained using 500 bootstrap replicates.

<<>>=
pr <- robpredict(mlfit, areameans=d, reps=500)
@

\noindent The results are
<<>>=
pr
@

\noindent and a visual display of the predicted county means

<<fig=TRUE>>=
plot(pr)
@


\noindent another plot, but with lines; here the predicted means are
sorted in ascending order
<<fig=TRUE>>=
plot(pr, type="l", sort="means")
@


Once the area-specific random effects have been predicted (by means
of robust methods), we can have a look at the residuals. Note that
the residuals are given by

\begin{equation}
r_{i,j}=y_{i,j} - \vx_{i,j}^T\hat{\vbeta}^{R} - \hat{u}_{i}^{R},
\end{equation}

\noindent where the superscript $R$ highlights the fact that robust
estimates/predictions are used. Obviously, since the residuals depend
on $\hat{u}_{i}^R$ (which is depends itself on the user-specified
robustness-tuning constant), a residual analysis can only take place
subsequent to robustly predicting $u_i$. This is contrast to the
standard ML (or REML) case where $\hat{u_i}$ is readily availabe having
estimated the fixed- and random-effects parameters.

The residualy may be used for an \texttt{QQ-plot}. (Note that
\texttt{bhfmodel} is not optimal in terms of the tail behavior
of the residuals)

<<fig=TRUE>>=
res <- residuals(pr)
qqnorm(res)
qqline(res)
@

%================================================
\section{Utility functions}

%------------------------------------------------
\subsection{Synthetic data generation}
The \texttt{rsae} package is shipped with a device to generate synthetic
data (balanced and unbalanced) of basic unit-level model. This tool is
particularly useful when comparing the behavior of different methods
(e.g., in simulation studies) because it enables the user to generate
data tailored for specific problems. In particular, \texttt{makedata}
is able to generate outlier-contaminated data.

%------------------------------------------------
\subsubsection{Uncontaminated data}

The core model is given by

\begin{equation}
    y_{i,j} = \alpha + \vx_{i,j}^T\vbeta + u_i + e_{i,j},
        \quad j=1,\ldots,n_i, \quad i=1, \ldots,g,
\end{equation}

\noindent where $y_{i,j}$ is the response variable, $\vx_{i,j}$ is a
$p$-vector of design variables, $u_i$ is an area-specific random effect,
and $e_{i,j}$ is a model error. The total number of observations
is $n=\sum_{i=1}^g n_i$. Moreover, we assume that $u_i$ and $e_{i,j}$
are independent and that

\begin{equation}
    \vx_{i,j} \sim MVN(\vzero, \mati_p),
\end{equation}

\noindent and

\begin{equation}
    e_{i,j} \sim N(0, v_e), \quad u_i \sim N(0, v_u),
\end{equation}

\noindent where $\mati_i$ is the $(p \times p)$ identity matrix,
$j=1,\ldots,n_i$; $i=1,\ldots,g$.

The default values of \texttt{makedata} for balanced data, i.e.,
$n_i=n, \forall i = 1,\ldots,g$ are reported in Table \ref{tab:makedata}.

\begin{table}[ht]
\caption{Default setup of \texttt{makedata} (balanced data)}
\centering
\begin{tabular}{p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}}
   \toprule
   \addlinespace
   $n$ &$g$ &$v_e$ &$v_u$ &$\alpha$ &$\beta$ \\
   \addlinespace
   \midrule
   $4$ &$20$ &$1$ &$1$ &$1$ &$1$ \\
   \bottomrule
\end{tabular}
\label{tab:makedata}
\end{table}

Now, call the \texttt{makedata} function

<<>>=
mymodel <- makedata()
@

\noindent to generate the data. Note that \texttt{makedata} has been
called without arguments, implying that all arguments are given by the
default value. However, you are well advised to specify the random
seed (default: \texttt{seed=1024}) to meet our needs. Otherwise, your
computer generates the same random numbers every time \texttt{makedata()}
is called.

By typing the name of the generated model (here \texttt{mymodel}) in
the console, we obtain (trough the \texttt{print} method) some
characteristics of the synthetic data. This feature is particularly
useful if we have several different models in the workspace (and have
some troubles figuring out what the models actually do).

<<>>=
mymodel
@

In addition, there is a \texttt{summary} method.

<<>>=
summary(mymodel)
@

%------------------------------------------------
\subsubsection{Contaminated data}
The \texttt{makedata} allows to draw the model errors $e_{i,j}$ and/or
random effects $u_i$ from a (contamination) mixture distribution.

\noindent Contamination of the law of $e_{i,j}$
<<>>=
makedata(ve.epsilon=0.1)
@

\noindent Contamination of the law of $u_{j}$

<<>>=
makedata(vu.epsilon=0.1)
@

\noindent and both

<<>>=
makedata(vu.epsilon=0.1, ve.epsilon=0.1)
@

\noindent It goes without saying that we may change other parameters
of the \texttt{makedata} function. For instance, we may generate
contaminated, unbalanced data; see below.


%------------------------------------------------
\subsubsection{Unbalanced data}
Further, we can generate unbalanced data very easily. First, we have
to set both arguments \texttt{n} and \texttt{g} in \texttt{makedata}
equal to \texttt{NULL} (since \texttt{n} is not a constant for
unbalanced data). Next, we have to tell \texttt{makedata} to produce
unbalanced data on grounds of a vector of area idetifiers (ID; all units
in a particular area are assigned an area-specific identifier code).
By way of example, suppose we want to generate 16 units that reside
in 5 areas. The following code snippet defines the vector identifiers.

<<>>=
my_area_id = c(1,1,1,1,1,2,2,3,3,3,4,4,4,4,4,5)
@

\noindent Thus, the vector of area size is
$nsize^T= (n_1, \ldots, n_5) = (5, 2, 3, 4, 1)^T$. (Note that the IDs
do not have to be sorted (as in the display); any permutation of the
IDs is a valid argument. The number of areas is determined from the
number of unique elements.)

The data are generated by the following command.

<<>>=
mymodelub <- makedata(n=NULL, g=NULL, areaID=my_area_id)
@

%================================================
%================================================
\appendix

%================================================
\section{About rsae}
<<>>=
citation("rsae")
@

%================================================
\section{Comparing the ML estimates of \texttt{rsae} with those of
\texttt{nlme}}

For ease of comparability, we report (again) the maximum likelihood
estimates obtained by \texttt{fitsaemodel}.

<<>>=
fitsaemodel("ml", bhfmodel)
@

\noindent These results are equal to those of the \texttt{lme}
function in the \textbf{nlme} package (which may be called the
industrial standard).

<<>>=
require(nlme)
nlme::lme(HACorn ~ PixelsCorn + PixelsSoybeans, random=~1 |
    CountyName, data=subset(landsat, subset=(outlier==FALSE)), method="ML")
@


%================================================
\section{Package installation on Linux systems}\label{sec:os}
In this section, I give some details on the installation of \texttt{rsae}
on a Linux powered system. These details essentially summarize my
experience with \texttt{openSUSE 11.4} on a \texttt{x86\_64} platform.
Some of the details carry directly over to other (mostly-) POSIX-compliant
systems, others do not. It is up to the user to modify these hints to fit
on his/her system. Note the following:

\begin{itemize}
    \item Make sure that you have installed \texttt{R-devel}
        (or \texttt{r-base-dev}) in addition to \texttt{R-base}; see
        \citet[][chapters 2, 6 and Appendix A.1]{radmin}
    \item The \texttt{rsae} package contains \texttt{FORTRAN 90} code
        that must be compiled (and linked to \texttt{R}'s \texttt{BLAS}
        and \texttt{LAPACK}) at installation. The code has been written
        and tested for \texttt{gfortran} (v. 4.5-1). The code does not
        use (to the best of my knowledge) any compiler-specific code.
        See \citet[][Appendix B.8]{radmin} for more details on
        \texttt{FORTRAN} compilers supported by \texttt{R}, and in what
        order they are selected if you have several compilers. Therefore,
        any compatible \texttt{FORTRAN} compiler should work. However, I
        highly recommend to use \texttt{gfortran}. (Note that if the
        \texttt{C} compiler that your current \texttt{R} installation
        knows of is \texttt{gcc 4} [the command \texttt{R CMD config CC}
        tells you what \texttt{R} uses; then check the version
        \texttt{gcc -v}], it will automatically choose \texttt{gfortran})
    \item you need the system tool \texttt{GNU make}. By default, most
        Linux distributions are delivered with a copy of \texttt{make},
        whereas \texttt{openSUSE 11.4} is not. You thus have to grab a
        version from the internet. See \citet[][Appendix B.5]{radmin}
        for more on \texttt{make}.
\end{itemize}

%================================================
% References

\begin{thebibliography}{9}
\bibitem[\protect\citeauthoryear{Copt and Victoria-Feser}
        {Copt and Victoria-Fester}{2009}]{coptvictoriafeser2009}
    Copt, S. and M.-P. Victoria-Feser (2009).
    \emph{Robust Predictions in Mixed Linear Models}.
    Research Report, University of Geneva.
\bibitem[\protect\citeauthoryear{Battese, Harter, and Fuller}
        {Battese et~al.}{1988}]{batteseetal1988}
    Battese, G.E., R.M. Harter, and W.A. Fuller (1988).
    \emph{An Error-Components Model for Prediction of County Crop
        Areas Using Survey and Satellite Data},
    Journal of the American Statistical Association 83, pp. 28-36.
\bibitem[\protect\citeauthoryear{Fellner}{Fellner}{1986}]{fellner1986}
    Fellner, W. (1986).
    \emph{Robust estimation of variance components}.
    Technometrics 28, pp. 51-60.
\bibitem[\protect\citeauthoryear{Heritier, Cantoni, Copt, and Victoria-Fester}
        {Heritier et~al.}{2009}]{heritieretal2009}
    Heritier, S., Cantoni, E., Copt, S., and M.-P. Victoria-Feser (2009).
    \emph{Robust methods in Biostatistics}.
    New York: John Wiley and Sons.
\bibitem[\protect\citeauthoryear{Pinheiro, Bates, Default, and Sarkar}
        {Pinheiro et~al.}{2011}]{pinheirobates2011}
    Pinheiro, J., D. Bates, S. DebRoy, D. Sarkar and the R
        Development Core Team (2011).
    \emph{nlme: Linear and Nonlinear Mixed Effects Models}.
    R package version 3.1-100.
\bibitem[\protect\citeauthoryear{R-admin}{R-admin}{2011}]{radmin}
    R-admin (2011).
    \emph{R Installation and Administration (version 2.13.0)},
    ed. by: R Development Core Team, Vienna.
    URL http.//stat.ethz.ch/CRAN/doc/manuals/R-admin.html.
\bibitem[\protect\citeauthoryear{R-exts}{R-exts}{2011}]{rexts}
    R-exts (2011).
    \emph{R Writing R Extensions (version 2.13.0)},
    ed. by: R Development Core Team, Vienna.
    URL http.//stat.ethz.ch/CRAN/doc/manuals/R-exts.html.
\bibitem[\protect\citeauthoryear{Rao}{Rao}{2003}]{rao2003}
    Rao, J.N.K.(2003).
    \emph{Small Area Estimation},
    New Work: John Wiley and Sons.
\bibitem[\protect\citeauthoryear{Richardson}{Richardson}{1997}]{richardson1997}
    Richardson, A.M. (1997).
    \emph{Bounded Influence Estimation in the Mixed Linear Model}.
    Journal of the American Statistical Association 92, pp.151-161
\bibitem[\protect\citeauthoryear{Richardson}{Richardson}{1995}]{richardson1995}
    Richardson, A.M. (1995).
    \emph{Some problems in estimation in mixed linear models}.
    PhD thesis, Australian National University.
\bibitem[\protect\citeauthoryear{Richardson and Welsh}{Richardson and Welsh}
        {1986}]{richardsonwelsh1995}
    Richardson, A.M. and A.H. Welsh (1995):
    \emph{Robust restricted maximum likelihood in mixed linear model}.
    Biometrics 51, pp. 1429-1439.
\bibitem[\protect\citeauthoryear{Rousseeuw}{Rousseeuw}{1984}]{rousseeuw1984}
    Rousseeuw, P.J. (1984).
    \emph{Least Median of Squares Regression}.
    Journal of the American Statistical Association 79, pp. 871-880
\bibitem[\protect\citeauthoryear{Rousseeuw and Yohai}{Rousseeuw and Yohai}
        {1984}]{rousseeuwyohai1984}
    Rousseeuw, P.J., and V.J. Yohai (1984).
    \emph{Robust regression by means of S-estimators},
    in: Robust and Nonlinear Time Series, Franke, J., W. H{\"ardle},
        and R.D. Martin (eds.),
    Lecture Notes in Statistics 26, pp. 256-272, New York: Springer.
\bibitem[\protect\citeauthoryear{Rousseeuw and van Driessen}
        {Rousseeuw and van Driessen}{2006}]{rousseeuwvandriessen2006}
    Rousseeuw, P.J., and K. Van Driessen (2006).
    \emph{Computing LTS Regression for Large Data Sets},
    Data Mining and Knowledge Discovery 12, pp. 29-45.
\bibitem[\protect\citeauthoryear{Rousseeuw Croux, Todorov, Ruckstuhl,
    Salibian-Barrera, Verbeke, Koller, Maechler}{Rousseeuw et~al.}{2011}]
        {robustbase}
    Rousseeuw, P.J., C. Croux, V. Todorov, A.  Ruckstuhl,
        M. Salibian-Barrera, T. Verbeke, M. Koller,  M. Maechler (2011).
    \emph{robustbase: Basic Robust Statistics}.
    R package version 0.7-6.
    URL http://CRAN.R-project.org/package=robustbase
\bibitem[\protect\citeauthoryear{Salibian-Barrera and Yohai}
        {Salibian-Barrera and Yohai}{2006}]{salibianbarerrayohai2006}
    Salibian-Barrera, M. and V.J. Yohai (2006).
    \emph{A fast algorithm for S-regression estimates}.
    Journal of Computational and Graphical Statistics 15, pp. 414-427.
\bibitem[\protect\citeauthoryear{Schoch}{Schoch}{2011}]{schoch2011}
    Schoch, T. (2011).
    \emph{The robust basic unit-level small area model. A simple and
        fast algorithm for large datasets},
    in: Proceedings of the New Technologies and Techniques
        Conference (NTTS), EUROSTAT, Brussels.
\bibitem[\protect\citeauthoryear{Sinha and Rao}{Sinha and Rao}{2009}]
        {sinharao2009}
    Sinha, S.K. and J.N.K. Rao (2009).
    \emph{Robust small area estimation}.
    Canadian Journal of Statistics 37, pp. 381-399.
\end{thebibliography}

%================================================
\end{document}
